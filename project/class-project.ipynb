{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Dataset Classification with Scikit-Learn and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll create a classifier using the fashion-MNIST dataset. We'll start with data exploration and a baseline model using Scikit-Learn's Logistic Regression. Then, we'll build a more advanced model using PyTorch to create a neural network. This step-by-step guide is designed to be completed in small sections of 5-10 minutes each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level outline\n",
    "\n",
    "### 1) Fetching, using, and manipulating datasets\n",
    "- Requires knowing sklearn, numpy, matplotlib, and pandas\n",
    "### 2) Baseline ML training with sklearn\n",
    "- Use sklearn to train and evaluate your model on the data\n",
    "### 3) Create your own SimpleNN and CNN\n",
    "- Use pytorch to pull in data, train your model and evaluate it\n",
    "### 4) Compare Sklearn and your custom models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources You should use for your project\n",
    "\n",
    "- [NumPy Documentation](https://numpy.org/doc/)\n",
    "- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [scikit-learn Documentation](https://scikit-learn.org/stable/documentation.html)\n",
    "- [PyTorch Documentation](https://pytorch.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Step 0 Install all necessary libraries](#step-0-install-all-necesssary-libraries) \n",
    "- [Step 1 Import Libraries and Load Dataset](#step-1-import-libraries-and-load-dataset)\n",
    "- [Step 2 Explore the dataset](#step-2-explore-the-dataset)\n",
    "- [Step 3 Preprocess and normalize the data](#step-3-preprocess-and-normalize-data)\n",
    "- [Step 4 Split the data into test and training](#step-4-split-data-into-training-and-testing-sets)\n",
    "- [Step 5 baseline model with sklearn](#step-5-baseline-model-with-scikit-learn-linear-models)\n",
    "- [Step 6 evalute the baseline model](#step-6-evaluate-baseline-model)\n",
    "- [Step 7 convert the data to pytorch tensors](#step-7-convert-data-to-pytorch-tensors)\n",
    "- [Step 8 Create DataLoaders](#step-8-create-dataloaders)\n",
    "- [Steo 9 Define custom NN with pytorch](#step-9-define-the-pytorch-neural-network-model)\n",
    "- [Step 10 Train your NN](#step-10-train-the-neural-network)\n",
    "- [Step 11 Visualize the training loss](#step-11-visualize-training-loss)\n",
    "- [Step 12 Evaluate the NN model](#step-12-evaluate-neural-network-model)\n",
    "- [Step 13 Create a confusion matrix](#step-13-confusion-matrix-and-sample-predictions)\n",
    "- [Step 14 Visualize classification vectors of custom NN](#step-14-visualizing-classification-vectors-of-a-neural-network)\n",
    "- [Step 15 Repeat steps 7-14 for CNN](#step-15-repeat-steps-7-14-for-a-custom-cnn-model)\n",
    "- [Step 16 Convert data to tensors](#step-16-convert-the-data-to-tensors)\n",
    "- [Step 17 Create DataLoaders](#step-17-create-data-loaders)\n",
    "- [Step 18 Define simple CNN model](#step-18-define-a-simple-cnn-model)\n",
    "- [Step 19 Train CNN](#step-19-train-the-cnn-model)\n",
    "- [Step 20 Visualize the training loss](#step-20-visualize-the-training-loss)\n",
    "- [Step 21 Evaluate the CNN model](#step-21-evaluate-the-cnn-model)\n",
    "- [Step 22 Create a confusion matrix](#step-22-plot-the-confusion-matrix-and-sample-predictions) \n",
    "- [Step 23 Visualize the classification vectors](#step-23-visualize-classification-vectors-for-the-cnn)\n",
    "- [Step 24 Display the feature maps for CNN](#step-24-display-the-feature-maps-for-your-convolutional-network)\n",
    "- [Step 25 Compare and contrast prebuilt models vs your custom models](#step-25-compare-the-different-models-accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Install all necesssary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Import Libraries and Load Dataset\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "First, we'll import all the necessary libraries and load the MNIST dataset from fetch_openml."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports Overview\n",
    "\n",
    "In this project, we use several powerful libraries and modules to handle data processing, build neural networks, and visualize results. Below is a breakdown of each library and module and how it contributes to our project.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `torch`\n",
    "The `torch` library is the core of the PyTorch framework, a popular open-source library for machine learning and deep learning. PyTorch provides efficient, flexible tensor operations that enable us to perform mathematical operations on multidimensional arrays (tensors), which are essential for neural networks. We use `torch` for creating and manipulating tensors that store our data and for performing the calculations required during neural network training.\n",
    "\n",
    "**From the docs:**\n",
    "The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.\n",
    "\n",
    "It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `torch.nn`\n",
    "The `torch.nn` module is part of PyTorch and provides tools for creating neural network layers and defining complex models. The `nn` module includes pre-built layers like fully connected (`Linear`), convolutional (`Conv2d`), and activation functions (`ReLU`). By using `torch.nn`, we can build each part of a neural network and chain them together to create the final model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `torch.optim`\n",
    "The `torch.optim` module provides various optimization algorithms for training neural networks. Optimization is a key step in machine learning, where we adjust the model’s parameters to minimize the error in predictions. `optim` contains optimizers such as **Stochastic Gradient Descent (SGD)** and **Adam**, which update the model's weights during training based on the gradients computed from the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. `torch.utils.data`\n",
    "The `torch.utils.data` module offers utilities for working with datasets, particularly in batching and loading data. It includes:\n",
    "   - **`DataLoader`**: Manages and batches data, making it easy to feed data into our model in small groups, which is essential for efficient training.\n",
    "   - **`TensorDataset`**: Wraps tensors into a dataset object, allowing us to combine data and labels into a single dataset that can be loaded by `DataLoader`.\n",
    "\n",
    "These tools help us efficiently load and prepare data for training and testing our model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. `sklearn.datasets.fetch_openml`\n",
    "`fetch_openml` is a function within the `sklearn.datasets` module, part of the Scikit-Learn library. It allows us to download popular datasets hosted on the [OpenML platform](https://www.openml.org/). In this project, we use `fetch_openml` to easily access and load the **MNIST dataset**, a collection of handwritten digits commonly used for training and testing image classification models.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. `numpy`\n",
    "`numpy` is a fundamental library for scientific computing in Python, providing support for efficient numerical operations on large, multidimensional arrays. In this project, we use `numpy` to handle data operations like reshaping images or performing mathematical calculations on arrays. PyTorch integrates well with `numpy`, allowing for smooth transitions between `numpy` arrays and PyTorch tensors, making it essential for data preparation and processing.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. `matplotlib.pyplot`\n",
    "`matplotlib.pyplot` is a submodule of Matplotlib, a popular Python plotting library. `plt` (as it’s commonly abbreviated) enables us to create data visualizations like line plots, bar charts, and images. In our project, we use `plt` to visualize training results, such as loss over time, and to display images from our dataset, which helps in understanding the model’s behavior and performance.\n",
    "\n",
    "---\n",
    "\n",
    "Each of these libraries plays a critical role in our project, from data handling and visualization to building and training the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "# AI libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# non AI libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the datasets we will be using today on this website: https://openml.org/search?type=data&status=active&id=40996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Load Fashion MNIST dataset using fetch_openml\n",
    "\n",
    "\n",
    "# print the description of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Explore the Dataset\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "Let's take a quick look at the dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data\n",
    "\n",
    "\n",
    "# Show the information about the data using info() method\n",
    "\n",
    "\n",
    "# Show the structure of the data using head() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the fist image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image in the dataset using matplotlibs imshow() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Preprocess and Normalize Data\n",
    "\n",
    "Estimated Time: 5 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Split Data into Training and Testing Sets\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "Split the data into training and testing sets using `train_test_split`. Set test_size and random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Baseline Model with Scikit-Learn linear models\n",
    "Estimated Time: 10 minutes\n",
    "\n",
    "Train the following 3 algorithms with the data:\n",
    "- Logistic Regression model. Use the `lbfgs` as your solver\n",
    "- Ridge Classifier\n",
    "- SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression on a subset of data\n",
    "\n",
    "# Ridge Classifier\n",
    "\n",
    "# SGD Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Evaluate Baseline Model\n",
    "Estimated Time: 10 minutes\n",
    "\n",
    "Predict on the test set and evaluate the baseline model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of each model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the classification vectors for the 3 different algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the coefficients for each classifier\n",
    "\n",
    "\n",
    "# Get the classes for each classifier\n",
    "\n",
    "\n",
    "# Get the intercepts for each classifier\n",
    "\n",
    "\n",
    "# Get the n_features for each classifier\n",
    "\n",
    "\n",
    "# Define the descriptive labels for each class\n",
    "\n",
    "\n",
    "# Plot the classification vectors for each classifier and each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to tensorflow\n",
    "\n",
    "In this section we will create two different custom made neural networks. The goal is to deepen your understanding about both the easy of python and the basics of machie learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Convert Data to PyTorch Tensors\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "Convert the pandas DataFrames and Series to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors for a simple neural network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Create DataLoaders\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "Create `TensorDataset` and `DataLoader` objects for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "\n",
    "# Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Define the PyTorch Neural Network Model\n",
    "Estimated Time: 10 minutes\n",
    "\n",
    "Define a simple feedforward neural network using PyTorch's `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "\n",
    "# Instantiate the model\n",
    "\n",
    "# Define loss function and optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Train the Neural Network\n",
    "Estimated Time: 10 minutes\n",
    "\n",
    "Train the neural network over several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11. Visualize Training Loss\n",
    "Estimated Time: 5 minutes\n",
    "\n",
    "Plot the training loss over epochs to see how the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss over epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12. Evaluate Neural Network Model\n",
    "Estimated Time: 10 minutes\n",
    "\n",
    "Evaluate the trained neural network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "\n",
    "# Calculate accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13. Confusion Matrix and Sample Predictions\n",
    "\n",
    "Estimated Time: 10 minutes\n",
    "\n",
    "Visualize the model's performance with a confusion matrix and sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Collect all predictions and true labels\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few test images with predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14. Visualizing Classification Vectors of a Neural Network\n",
    "\n",
    "In this task, you will visualize the classification vectors of a neural network by extracting the weights from the final layer and reshaping them to match the input dimensions. This helps in understanding how the neural network differentiates between different classes based on the learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights from the final layer\n",
    "\n",
    "# Plot the classification vectors for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15 Repeat steps 7-14 for a custom CNN model\n",
    "We want to test how a CNN would perform compared to our simple Neural Network\n",
    "\n",
    "## Step 16 Convert the data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for CNN\n",
    "\n",
    "# Convert data to tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17 Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets\n",
    "\n",
    "\n",
    "# Create DataLoaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18 Define a simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "\n",
    "# Instantiate the CNN model\n",
    "\n",
    "# Define loss function and optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19 Train the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20 Visualize the Training Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss over epochs for CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21 Evaluate the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the CNN model on the test set\n",
    "\n",
    "# Calculate accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22 Plot the Confusion matrix and Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all predictions and true labels for CNN\n",
    "\n",
    "# Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few test images with predictions from CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23. Visualize Classification Vectors for the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the weights from the final layer\n",
    "\n",
    "# Plot the classification vectors for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 24 Display the feature maps for your convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an input image\n",
    "\n",
    "# Forward pass through conv1\n",
    "\n",
    "# Plot the feature maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 25. Compare the different models accuracy\n",
    "\n",
    "Estimated Time: 5 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
