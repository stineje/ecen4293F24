{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBcKjJ9EpbTa"
      },
      "source": [
        "# ECEN 4293:  Interactive PyTorch Operations with Colab\n",
        "In this notebook, you will practice some basic operations in PyTorch, such as addition, matrix multiplication, and other tensor operations. Each section will ask you to perform a specific operation, and you'll write the code to perform the task.\n",
        "\n",
        "Let's begin!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PyTorch\n",
        "\n",
        "PyTorch is an open-source deep learning framework developed by Facebook’s AI Research lab (FAIR). It is widely used for building and training machine learning models, particularly neural networks, due to its ease of use, flexibility, and dynamic computational graph system.\n",
        "\n",
        "### Summary of PyTorch Advantages:\n",
        "1.\tDynamic Computation Graphs: Easier to debug and more flexible compared to static frameworks.\n",
        "2.\tAutomatic Differentiation: Built-in support for computing gradients and backpropagation.\n",
        "3.\tGPU Acceleration: Easy to move computations to the GPU.\n",
        "4.\tModular and Flexible: Extensible modules for building complex neural networks.\n",
        "5.\tLarge Ecosystem: Libraries for computer vision, NLP, audio processing, and more.\n",
        "6.\tResearch-Oriented: Widely used in academia and research.\n",
        "7.\tStrong Python Integration: Works seamlessly with Python and Python-based tools.\n",
        "\n",
        "### Use on Colab\n",
        "\n",
        "Google Colab is a great environment to run PyTorch code because it provides free access to GPUs and TPUs for accelerated computation, making it ideal for deep learning tasks.\n",
        "\n",
        "Using PyTorch on Google Colab is a highly effective way to train models, experiment with deep learning, and leverage free GPU or TPU resources. With easy access to hardware accelerators, integration with Google Drive, and pre-installed libraries, Colab makes it very convenient for both beginners and experienced researchers to build, train, and test models.\n",
        "\n",
        "### Using GPUs on Colab\n",
        "\n",
        "By default, Colab runs on a CPU. To use a GPU for accelerated computation, you need to switch the runtime to use a GPU.\n",
        "\n",
        "Steps to Enable GPU:\n",
        "\n",
        "1.\tGo to Runtime > Change runtime type.\n",
        "2.\tIn the popup window, under Hardware accelerator, choose GPU.\n",
        "3.\tClick Save.\n",
        "\n",
        "### What are TPUs\n",
        "\n",
        "Google Colab also provides access to TPUs (Tensor Processing Units), which are optimized for training large machine learning models. However, working with TPUs in PyTorch requires the use of torch_xla, a package specifically designed for PyTorch-TPU integration.  \n",
        "\n",
        "### Setting up for PyTorch\n",
        "\n",
        "Setting up PyTorch is relatively straightforward and can be done in various environments depending on your needs (e.g., local machine, cloud services like Google Colab, or a virtual environment).\n"
      ],
      "metadata": {
        "id": "-zlOwKCz2j4Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "A5tLX92LpbTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c0cbd8-dbb5-4d46-f7ed-21fee07a0f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using torch 2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# Basic Setup\n",
        "import torch\n",
        "print(\"Using torch\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the time of writing this tutorial (October 2024), the current stable version is 2.4. You should therefore see the output Using torch 2.4.1, eventually with some extension for the CUDA version on Colab. In case you use the dl2024 environment, you should see Using torch 2.4.1. In general, it is recommended to keep the PyTorch version updated to the newest one. If you see a lower version number than 2.3, make sure you have installed the correct environment, or ask us. In case PyTorch 2.5 or newer will be published during the time of the course, don't worry. The interface between PyTorch versions doesn't change too much, and hence all code should also be runnable with newer versions.\n",
        "As in every machine learning framework, PyTorch provides functions that are stochastic like generating random numbers. However, a very good practice is to setup your code to be reproducible with the exact same random numbers. This is why we set a seed below."
      ],
      "metadata": {
        "id": "CYP8qZuKB2NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42) # Setting the seed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vY6P97LB_XZ",
        "outputId": "6b9745bc-6fc8-4616-9f8b-469b68f15d8c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7daa281c6590>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors\n",
        "\n",
        "Tensors are the PyTorch equivalent to Numpy arrays, with the addition to also have support for GPU acceleration (more on that later).\n",
        "The name \"tensor\" is a generalization of concepts you already know. For instance, a vector is a 1-D tensor, and a matrix a 2-D tensor. When working with neural networks, we will use tensors of various shapes and number of dimensions.\n",
        "\n",
        "Most common functions you know from numpy can be used on tensors as well. Actually, since numpy arrays are so similar to tensors, we can convert most tensors to numpy arrays (and back) but we don't need it too often.\n",
        "\n",
        "#### Initialization\n",
        "\n",
        "Let's first start by looking at different ways of creating a tensor. There are many possible options, the simplest one is to call `torch.Tensor` passing the desired shape as input argument:"
      ],
      "metadata": {
        "id": "qmmmwRl1CUIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.Tensor(2, 3, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cKZpbmCCX-q",
        "outputId": "6ff68f96-9eb9-403d-89e8-a413e1aca728"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 8.3382e-10,  4.2957e-05,  1.6536e-04,  1.2812e-11],\n",
            "         [ 3.2918e-09,  3.2505e+21,  1.3353e-08,  1.2752e+16],\n",
            "         [ 1.3452e-43,  0.0000e+00,  1.3452e-43,  0.0000e+00]],\n",
            "\n",
            "        [[-1.7598e-26,  7.0065e-45,  5.0447e-44,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  5.3179e+22,  2.0943e+23,  5.2476e-08]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `torch.Tensor` allocates memory for the desired tensor, but reuses any values that have already been in the memory. To directly assign values to the tensor during initialization, there are many alternatives including:\n",
        "\n",
        "* `torch.zeros`: Creates a tensor filled with zeros\n",
        "* `torch.ones`: Creates a tensor filled with ones\n",
        "* `torch.rand`: Creates a tensor with random values uniformly sampled between 0 and 1\n",
        "* `torch.randn`: Creates a tensor with random values sampled from a normal distribution with mean 0 and variance 1\n",
        "* `torch.arange`: Creates a tensor containing the values $N,N+1,N+2,...,M$\n",
        "* `torch.Tensor` (input list): Creates a tensor from the list elements you provide"
      ],
      "metadata": {
        "id": "iVgiCD-tCdBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor from a (nested) list\n",
        "x = torch.Tensor([[1, 2], [3, 4]])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf_qVD1gCi5Y",
        "outputId": "c0359f2d-68ab-4b40-fb6d-33feba4a73d1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor with random values between 0 and 1 with the shape [2, 3, 4]\n",
        "x = torch.rand(2, 3, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGainh8UClue",
        "outputId": "66be3083-4c76-448a-e63a-e587ea93dd64"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.8823, 0.9150, 0.3829, 0.9593],\n",
            "         [0.3904, 0.6009, 0.2566, 0.7936],\n",
            "         [0.9408, 0.1332, 0.9346, 0.5936]],\n",
            "\n",
            "        [[0.8694, 0.5677, 0.7411, 0.4294],\n",
            "         [0.8854, 0.5739, 0.2666, 0.6274],\n",
            "         [0.2696, 0.4414, 0.2969, 0.8317]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can obtain the shape of a tensor in the same way as in numpy (`x.shape`), or using the `.size` method:"
      ],
      "metadata": {
        "id": "2OdA8OSlCroD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape = x.shape\n",
        "print(\"Shape:\", x.shape)\n",
        "\n",
        "size = x.size()\n",
        "print(\"Size:\", size)\n",
        "\n",
        "dim1, dim2, dim3 = x.size()\n",
        "print(\"Size:\", dim1, dim2, dim3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moQybiJCCvy4",
        "outputId": "ee021ff5-f79a-472e-bcd5-bcadc4b96c77"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: torch.Size([2, 3, 4])\n",
            "Size: torch.Size([2, 3, 4])\n",
            "Size: 2 3 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensor to Numpy, and Numpy to Tensor\n",
        "\n",
        "Tensors can be converted to numpy arrays, and numpy arrays back to tensors. To transform a numpy array into a tensor, we can use the function `torch.from_numpy`:"
      ],
      "metadata": {
        "id": "Ev7Ftp0VC1PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_arr = np.array([[1, 2], [3, 4]])\n",
        "tensor = torch.from_numpy(np_arr)\n",
        "\n",
        "print(\"Numpy array:\", np_arr)\n",
        "print(\"PyTorch tensor:\", tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK2XTfJIC5Y5",
        "outputId": "a207aa59-1790-4ced-f873-1f26f81a129d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numpy array: [[1 2]\n",
            " [3 4]]\n",
            "PyTorch tensor: tensor([[1, 2],\n",
            "        [3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*To* transform a PyTorch tensor back to a numpy array, we can use the function `.numpy()` on tensors:"
      ],
      "metadata": {
        "id": "OkmLsIfiC_1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.arange(4)\n",
        "np_arr = tensor.numpy()\n",
        "\n",
        "print(\"PyTorch tensor:\", tensor)\n",
        "print(\"Numpy array:\", np_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkaL4mKLDGVm",
        "outputId": "4e7ee84c-85a3-435b-df1a-18fbbf44f157"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch tensor: tensor([0, 1, 2, 3])\n",
            "Numpy array: [0 1 2 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Operations\n",
        "\n",
        "Most operations that exist in numpy, also exist in PyTorch. A full list of operations can be found in the [PyTorch documentation](https://pytorch.org/docs/stable/tensors.html#), but we will review the most important ones here.\n",
        "\n",
        "The simplest operation is to add two tensors:"
      ],
      "metadata": {
        "id": "rBKHQiOWDQb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.rand(2, 3)\n",
        "x2 = torch.rand(2, 3)\n",
        "y = x1 + x2\n",
        "\n",
        "print(\"X1\", x1)\n",
        "print(\"X2\", x2)\n",
        "print(\"Y\", y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOuuOqxuDUWb",
        "outputId": "78f0bf04-76eb-4e02-fda0-82d9c46002f9"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 tensor([[0.1053, 0.2695, 0.3588],\n",
            "        [0.1994, 0.5472, 0.0062]])\n",
            "X2 tensor([[0.9516, 0.0753, 0.8860],\n",
            "        [0.5832, 0.3376, 0.8090]])\n",
            "Y tensor([[1.0569, 0.3448, 1.2448],\n",
            "        [0.7826, 0.8848, 0.8151]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling `x1 + x2` creates a new tensor containing the sum of the two inputs. However, we can also use in-place operations that are applied directly on the memory of a tensor. We therefore change the values of `x2` without the chance to re-accessing the values of `x2` before the operation. An example is shown below:"
      ],
      "metadata": {
        "id": "JeVVdGQDDbFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = torch.rand(2, 3)\n",
        "x2 = torch.rand(2, 3)\n",
        "print(\"X1 (before)\", x1)\n",
        "print(\"X2 (before)\", x2)\n",
        "\n",
        "x2.add_(x1)\n",
        "print(\"X1 (after)\", x1)\n",
        "print(\"X2 (after)\", x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_6TGsIjDeQD",
        "outputId": "c1c8e5f3-cc55-4aa7-ead7-939344737e87"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1 (before) tensor([[0.5779, 0.9040, 0.5547],\n",
            "        [0.3423, 0.6343, 0.3644]])\n",
            "X2 (before) tensor([[0.7104, 0.9464, 0.7890],\n",
            "        [0.2814, 0.7886, 0.5895]])\n",
            "X1 (after) tensor([[0.5779, 0.9040, 0.5547],\n",
            "        [0.3423, 0.6343, 0.3644]])\n",
            "X2 (after) tensor([[1.2884, 1.8504, 1.3437],\n",
            "        [0.6237, 1.4230, 0.9539]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In-place operations are usually marked with a underscore postfix (e.g. \"add_\" instead of \"add\").\n",
        "\n",
        "Another common operation aims at changing the shape of a tensor. A tensor of size (2,3) can be re-organized to any other shape with the same number of elements (e.g. a tensor of size (6), or (3,2), ...). In PyTorch, this operation is called `view`:"
      ],
      "metadata": {
        "id": "11CvXvswDir-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(6)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS4vZiV4DmWn",
        "outputId": "9d5ef660-4c0f-44d8-c2db-028e69522e46"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([0, 1, 2, 3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.view(2, 3)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S89hNUlrDojq",
        "outputId": "ba41072b-4b40-4663-9c23-892b2ee88816"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.permute(1, 0) # Swapping dimension 0 and 1\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0zunYPZDsus",
        "outputId": "6ac3b1ff-1278-4b23-9c97-a4fb5eee1a9b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[0, 3],\n",
            "        [1, 4],\n",
            "        [2, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other commonly used operations (discussed in class) include matrix multiplications, which are essential for neural networks. Quite often, we have an input vector $\\mathbf{x}$, which is transformed using a learned weight matrix $\\mathbf{W}$. There are multiple ways and functions to perform matrix multiplication, some of which we list below:\n",
        "\n",
        "* `torch.matmul`: Performs the matrix product over two tensors, where the specific behavior depends on the dimensions. If both inputs are matrices (2-dimensional tensors), it performs the standard matrix product. For higher dimensional inputs, the function supports broadcasting (for details see the [documentation](https://pytorch.org/docs/stable/generated/torch.matmul.html?highlight=matmul#torch.matmul)). Can also be written as `a @ b`, similar to numpy.\n",
        "* `torch.mm`: Performs the matrix product over two matrices, but doesn't support broadcasting (see [documentation](https://pytorch.org/docs/stable/generated/torch.mm.html?highlight=torch%20mm#torch.mm))\n",
        "* `torch.bmm`: Performs the matrix product with a support batch dimension. If the first tensor $T$ is of shape ($b\\times n\\times m$), and the second tensor $R$ ($b\\times m\\times p$), the output $O$ is of shape ($b\\times n\\times p$), and has been calculated by performing $b$ matrix multiplications of the submatrices of $T$ and $R$: $O_i = T_i @ R_i$\n",
        "* `torch.einsum`: Performs matrix multiplications and more (i.e. sums of products) using the Einstein summation convention.\n",
        "\n",
        "Usually, we use `torch.matmul` or `torch.bmm`. We can try a matrix multiplication with `torch.matmul` below."
      ],
      "metadata": {
        "id": "S0rPlxbvDvlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(6)\n",
        "x = x.view(2, 3)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvaC7E7TD1m8",
        "outputId": "33d630b0-3106-47b8-bf19-a3b66ada80f4"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[0, 1, 2],\n",
            "        [3, 4, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.arange(9).view(3, 3) # We can also stack multiple operations in a single line\n",
        "print(\"W\", W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIxrsN_-D3r3",
        "outputId": "1b6c8f4d-6e1a-46e2-c9cf-74dcd65d6e72"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W tensor([[0, 1, 2],\n",
            "        [3, 4, 5],\n",
            "        [6, 7, 8]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = torch.matmul(x, W) # Verify the result by calculating it by hand too!\n",
        "print(\"h\", h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nbr6WFJD5oW",
        "outputId": "d1320e27-2bb4-481e-9bd5-e9446f21e15c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h tensor([[15, 18, 21],\n",
            "        [42, 54, 66]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Indexing\n",
        "\n",
        "We often have the situation where we need to select a part of a tensor. Indexing works just like in numpy, so let's try it:"
      ],
      "metadata": {
        "id": "ptwk39U4D-wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(12).view(3, 4)\n",
        "print(\"X\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRIzykEzEB6T",
        "outputId": "caf108d3-ca4b-497f-e808-5f9868916b38"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:, 1])   # Second column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbpl73NnEE3u",
        "outputId": "497c9664-997a-4a44-e90e-a70773757373"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 5, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[:2, -1]) # First two rows, last column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eerl1g8ZEHTv",
        "outputId": "7d1b8864-3310-419e-ba1c-263723724b73"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[1:3, :]) # Middle two rows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb0aNy9FEJNH",
        "outputId": "5e1ff08d-cbe1-4e6b-8938-d6cdb4b40d69"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# October 14, 2024 work starts here"
      ],
      "metadata": {
        "id": "VdU3nwSLKY5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you begin, let's try a traditional matrix multiply with NumPy too.\n",
        "\n",
        "## 0. Matrix Multiply with CPU with NumPy\n",
        "\n",
        "**Exercise 1:** Calculate $-2E$, $G+F$, $4F-G$, $HG$, and $GE$ using the following matrix definitions.  Do the exercise on paper first, then check by doing the calculation with NumPy arrays.  Let's use the randomization function in NumPy to help."
      ],
      "metadata": {
        "id": "Bo5TMzgSbNkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Code solution here\n",
        "import numpy as np\n",
        "\n",
        "# Create a random matrix of size (rows x cols)\n",
        "E = np.random.randn(2, 1)\n",
        "print(E, '\\n', sep='')\n",
        "\n",
        "F = np.random.randn(3, 2)\n",
        "print(F, '\\n', sep='')\n",
        "\n",
        "G = np.random.randn(3, 2)\n",
        "print(G, '\\n', sep='')\n",
        "\n",
        "H = np.random.randn(3, 3)\n",
        "print(H, '\\n', sep='')\n"
      ],
      "metadata": {
        "id": "KXH5ivRNbXXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-MmfZ2bpbTc"
      },
      "source": [
        "## 1. Tensor Addition\n",
        "**Task**: Create two 2x2 tensors and add them together using PyTorch.\n",
        "\n",
        "Write your solution below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "MwH6Zds0pbTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2360b88-9183-4b44-fb2a-1ece804cdc07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor A:\n",
            " tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Tensor B:\n",
            " tensor([[5, 6],\n",
            "        [7, 8]])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create two 2x2 tensors and add them together\n",
        "A = torch.tensor([[1, 2], [3, 4]])\n",
        "B = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "print(\"Tensor A:\\n\", A)\n",
        "print(\"Tensor B:\\n\", B)\n",
        "\n",
        "# Add the tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXQLHzkJpbTd"
      },
      "source": [
        "## 2. Matrix Multiplication\n",
        "**Task**: Create two 2x3 and 3x2 tensors and multiply them together using matrix multiplication in PyTorch.\n",
        "\n",
        "Write your solution below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "s_R6vF7tpbTd"
      },
      "outputs": [],
      "source": [
        "# TODO: Create two tensors (A: 2x3 and B: 3x2) and multiply them\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "B = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
        "\n",
        "# Perform matrix multiplication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NRIvykspbTd"
      },
      "source": [
        "## 3. Element-wise Multiplication\n",
        "**Task**: Create two 2x3 tensors and perform element-wise multiplication.\n",
        "\n",
        "Write your solution below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "846W_s6dpbTd"
      },
      "outputs": [],
      "source": [
        "# TODO: Create two tensors and perform element-wise multiplication\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "B = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
        "\n",
        "# Perform element-wise multiplication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNeFdl1kpbTd"
      },
      "source": [
        "## 4. Tensor Reshaping\n",
        "**Task**: Create a tensor of shape (2x4) and reshape it into a (4x2) tensor using PyTorch's `view()` function.\n",
        "\n",
        "Write your solution below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "1JtFtIMVpbTe"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a 2x4 tensor and reshape it to 4x2\n",
        "A = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "\n",
        "# Reshape the tensor using the view function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1r77TDrpbTe"
      },
      "source": [
        "## 5. Tensor Broadcasting\n",
        "**Task**: Use PyTorch's broadcasting feature to add a 1x3 tensor to a 3x3 tensor.  Again, tensor Broadcasting in PyTorch (and other deep learning frameworks like NumPy) refers to a set of rules that allow for element-wise operations on tensors of different shapes, without explicitly replicating data.\n",
        "\n",
        "Broadcasting enables PyTorch to perform arithmetic operations (like addition, multiplication, etc.) on tensors that don’t have exactly the same shape, but are compatible according to specific broadcasting rules.\n",
        "\n",
        "### Broadcasting Rules\n",
        "\n",
        "When performing operations on tensors with different shapes, PyTorch automatically broadcasts them to a common shape by following these rules:\n",
        "\n",
        "1.\tIf the tensors have a different number of dimensions, the one with fewer dimensions is padded with ones on the left side until they have the same number of dimensions.\n",
        "2.\tTensors are compatible when their dimensions match or one of the dimensions is 1. If a dimension is 1 in one tensor and a larger value in the other tensor, the smaller tensor is “stretched” along that axis to match the larger one.\n",
        "3.\tOnce the tensors are compatible, PyTorch performs the operation element-wise as if the smaller tensor had been expanded to match the shape of the larger tensor. However, no actual copying of data occurs, which is efficient both in terms of speed and memory.\n",
        "\n",
        "### Advantages/Disadvantages\n",
        "\n",
        "*\tAdvantage: PyTorch tensors support broadcasting, which allows for operations on tensors of different shapes without explicitly reshaping them. This capability simplifies code and enhances performance when dealing with tensors of different sizes.\n",
        "*\tDifference: While NumPy also supports broadcasting, PyTorch allows this functionality to extend to deep learning tasks and GPU-accelerated computations.\n",
        "\n",
        "Write your solution below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "DfRgNfsDpbTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c0d43d-fe1d-4562-8967-a80a25e35aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcasted Addition:\n",
            " tensor([[11, 22, 33],\n",
            "        [14, 25, 36],\n",
            "        [17, 28, 39]])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Use broadcasting to add a 1x3 tensor to a 3x3 tensor\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "B = torch.tensor([10, 20, 30])\n",
        "\n",
        "# Add with broadcasting\n",
        "C = A + B\n",
        "print(\"Broadcasted Addition:\\n\", C)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Batch Matrix Multiplication\n",
        "**Task**: Batch matrix multiplication is used to perform matrix multiplication on batched 3D tensors. Each slice along the batch dimension is treated as a separate matrix multiplication, and it applies matrix multiplication to all the slices at once.\n",
        "\n",
        "The input tensors to torch.bmm must be 3-dimensional, where:\n",
        "\n",
        "*\tThe first dimension is the batch size.\n",
        "*\tThe second and third dimensions represent the matrices to be multiplied.\n",
        "\n",
        "The shape of the tensors should be (batch_size, n, m) and (batch_size, m, p). The result will have shape (batch_size, n, p).\n",
        "\n",
        "Write your solution below:"
      ],
      "metadata": {
        "id": "fA4eZVoLK2ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two 3D tensors (batch_size, n, m) and (batch_size, m, p)\n",
        "# A is of shape (2, 2, 3) -> two 2x3 matrices\n",
        "A = torch.tensor([[[1, 2, 3], [4, 5, 6]],    # First 2x3 matrix\n",
        "                  [[7, 8, 9], [10, 11, 12]]]) # Second 2x3 matrix\n",
        "\n",
        "# B is of shape (2, 3, 2) -> two 3x2 matrices\n",
        "B = torch.tensor([[[1, 4], [2, 5], [3, 6]],    # First 3x2 matrix\n",
        "                  [[7, 10], [8, 11], [9, 12]]]) # Second 3x2 matrix\n",
        "\n",
        "# Perform batch matrix multiplication (bmm)"
      ],
      "metadata": {
        "id": "lbEzdSqFK9LG"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Einstein Summation\n",
        "\n",
        "Einstein summation is a powerful and flexible function in PyTorch that allows for complex tensor operations using Einstein summation notation. It is useful for performing operations such as tensor contraction, transpose, inner products, and outer products with minimal code.\n",
        "\n",
        "Here’s how the Einstein summation convention works:\n",
        "\n",
        "*\tThe subscripts represent the axes of the tensors.\n",
        "*\tThe same subscript between two tensors implies summation along that axis (like matrix multiplication).\n",
        "*\tUnmatched subscripts represent the axes to be retained in the output.\n",
        "\n",
        "### Notation for Matrix Multiplication\n",
        "\n",
        "The notation 'ik,kj->ij' in the torch.einsum function corresponds directly to the mathematical Einstein summation convention for matrix multiplication:\n",
        "* ik: Refers to the two dimensions of matrix A (rows i, columns k).\n",
        "* kj: Refers to the two dimensions of matrix B (rows k, columns j).\n",
        "* ->ij: This indicates that the output will have dimensions i and j (the rows of A and the columns of B).\n",
        "* This notation effectively performs matrix multiplication by summing over the k axis.\n",
        "\n",
        "### Outer Product\n",
        "\n",
        "The outer product of two vectors results in a matrix, where each element is the product of corresponding elements from the two vectors.\n",
        "\n",
        "The notation 'i,j->ij' means:\n",
        "*\ti: Refers to the first vector A.\n",
        "*\tj: Refers to the second vector B.\n",
        "*\t->ij: This indicates that the output will have dimensions i and j (forming a matrix).\n",
        "\n",
        "The result is a matrix where each element is the product of corresponding elements from A and B.\n",
        "\n",
        "### Summation over Specific Axes\n",
        "\n",
        "You can also use torch.einsum to sum over specific axes of a tensor.\n",
        "\n",
        "The notation 'ij->i' means:\n",
        "*\tij: Refers to a 2D tensor (matrix A).\n",
        "*\t->i: This means we sum over the j axis (columns) to keep only the i axis (rows).\n"
      ],
      "metadata": {
        "id": "vHawl4mZOPgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two 2D tensors (matrices)\n",
        "A = torch.tensor([[1, 2], [3, 4]])  # Shape: (2, 2)\n",
        "B = torch.tensor([[5, 6], [7, 8]])  # Shape: (2, 2)\n",
        "\n",
        "# Matrix multiplication using einsum\n",
        "\n",
        "# Outer product using einsum\n",
        "A = torch.tensor([1, 2, 3])  # Shape: (3)\n",
        "B = torch.tensor([4, 5, 6])  # Shape: (3)\n",
        "\n",
        "# Sum over the rows (axis 0)\n",
        "A = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape: (2, 3)"
      ],
      "metadata": {
        "id": "rsvKboY_O-Ke"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdpizfGPpbTe"
      },
      "source": [
        "## 8. Create Random Tensors\n",
        "**Task**: Create two random tensors of shape (3x3) and add them together. Use PyTorch's `torch.rand()` to generate random values between 0 and 1.\n",
        "\n",
        "Write your solution below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "9JnT-wN0pbTf"
      },
      "outputs": [],
      "source": [
        "# TODO: Create two random 3x3 tensors and add them together\n",
        "A = torch.rand(3, 3)\n",
        "B = torch.rand(3, 3)\n",
        "\n",
        "# Add the random tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU support\n",
        "\n",
        "A crucial feature of PyTorch is the support of GPUs, short for Graphics Processing Unit. A GPU can perform many thousands of small operations in parallel, making it very well suitable for performing large matrix operations in neural networks. When comparing GPUs to CPUs, we can list the following main differences (credit: [Kevin Krewell, 2009](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/))\n",
        "\n",
        "\n",
        "\n",
        "CPUs and GPUs have both different advantages and disadvantages, which is why many computers contain both components and use them for different tasks. In case you are not familiar with GPUs, you can read up more details in this [NVIDIA blog post](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/) or [here](https://www.intel.com/content/www/us/en/products/docs/processors/what-is-a-gpu.html).\n",
        "\n",
        "$$\n",
        "\\begin{array}{|c|c|c|c|c|c|c|}\\hline\n",
        "\\text{CPU} & \\text{GPU} \\\\ \\hline \\hline\n",
        "\\text{Central Processing Unit} & \\text{Graphics Processing Unit} \\\\ \\hline\n",
        "\\text{Several Cores} & \\text{Many cores} \\\\ \\hline\n",
        "\\text{Low latency} & \\text{High throughput (lots of concurrency)} \\\\ \\hline\n",
        "\\text{Good for serial processing} & \\text{Good for parallel processing} \\\\ \\hline\n",
        "\\text{Can do a handful of operations at once} & \\text{Can do thousands of operations at once} \\\\ \\hline\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "GPUs can accelerate the training of your network up to a factor of $100$ which is essential for large neural networks. PyTorch implements a lot of functionality for supporting GPUs (mostly those of NVIDIA due to the libraries [CUDA](https://developer.nvidia.com/cuda-zone) and [cuDNN](https://developer.nvidia.com/cudnn))."
      ],
      "metadata": {
        "id": "m63_22BFFY8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's check whether you have a GPU available:"
      ],
      "metadata": {
        "id": "-9g8MZlVGVdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_avail = torch.cuda.is_available()\n",
        "print(f\"Is the GPU available? {gpu_avail}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOUpnzOrGMfv",
        "outputId": "b9cd5a40-b43e-43e3-bdd8-f3c7799869f4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is the GPU available? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have a GPU on your computer but the command above returns False, make sure you have the correct CUDA-version installed. The `dl2024` environment comes with the CUDA 12.4, which is selected for the Snellius supercomputer. Please change it if necessary. On Google Colab, make sure that you have selected a GPU in your runtime setup (in the menu, check under `Runtime -> Change runtime type`).\n",
        "\n",
        "By default, all tensors you create are stored on the CPU. We can push a tensor to the GPU by using the function `.to(...)`, or `.cuda()`. However, it is often a good practice to define a `device` object in your code which points to the GPU if you have one, and otherwise to the CPU. Then, you can write your code with respect to this device object, and it allows you to run the same code on both a CPU-only system, and one with a GPU. Let's try it below. We can specify the device as follows:"
      ],
      "metadata": {
        "id": "R_Ps_CsyGZBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9CLUARFGdTa",
        "outputId": "839ebc5f-dfae-4d79-827b-4168b996c0bd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you have a GPU, you should now see the attribute `device='cuda:0'` being printed next to your tensor. The zero next to cuda indicates that this is the zero-th GPU device on your computer. PyTorch also supports multi-GPU systems, but this you will only need once you have very big networks to train (if interested, see the [PyTorch documentation](https://pytorch.org/docs/stable/distributed.html#distributed-basics)). We can also compare the runtime of a large matrix multiplication on the CPU with a operation on the GPU:"
      ],
      "metadata": {
        "id": "9NaZqHd-GkWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "x = torch.randn(5000, 5000)\n",
        "\n",
        "## CPU version\n",
        "start_time = time.time()\n",
        "_ = torch.matmul(x, x)\n",
        "end_time = time.time()\n",
        "print(f\"CPU time: {(end_time - start_time):6.5f}s\")\n",
        "\n",
        "## GPU version\n",
        "x = x.to(device)\n",
        "_ = torch.matmul(x, x)  # First operation to 'burn in' GPU\n",
        "# CUDA is asynchronous, so we need to use different timing functions\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "start.record()\n",
        "_ = torch.matmul(x, x)\n",
        "end.record()\n",
        "torch.cuda.synchronize()  # Waits for everything to finish running on the GPU\n",
        "print(f\"GPU time: {0.001 * start.elapsed_time(end):6.5f}s\")  # Milliseconds to seconds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "ZtphJxCcGs4P",
        "outputId": "2099c42a-9fe9-482b-a6f5-141d63d8c47c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU time: 3.82142s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-94aa8d0b94d7>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menable_timing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menable_timing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/streams.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mcurrent_stream\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \"\"\"\n\u001b[0;32m--> 917\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     streamdata = torch._C._cuda_getCurrentStream(\n\u001b[1;32m    919\u001b[0m         \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the size of the operation and the CPU/GPU in your system, the speedup of this operation can be >50x. As `matmul` operations are very common in neural networks, we can already see the great benefit of training a NN on a GPU. The time estimate can be relatively noisy here because we haven't run it for multiple times. Feel free to extend this, but it also takes longer to run.\n",
        "\n",
        "When generating random numbers, the seed between CPU and GPU is not synchronized. Hence, we need to set the seed on the GPU separately to ensure a reproducible code. Note that due to different GPU architectures, running the same code on different GPUs does not guarantee the same random numbers. Still, we don't want that our code gives us a different output every time we run it on the exact same hardware. Hence, we also set the seed on the GPU:"
      ],
      "metadata": {
        "id": "SilXpWU9Gp_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU operations have a separate seed we also want to set\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
        "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "gBW9O8dNGxVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs555eIHpbTf"
      },
      "source": [
        "## Conclusion\n",
        "You've now practiced some of the basic operations in PyTorch, such as matrix multiplication, element-wise operations, reshaping, and broadcasting. PyTorch makes these operations easy to perform, even with more complex models and tensors. Keep exploring PyTorch by experimenting with more tensor operations and applying them to real-world tasks!"
      ]
    }
  ]
}